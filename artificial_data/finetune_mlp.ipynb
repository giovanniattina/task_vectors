{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cf1d36",
   "metadata": {},
   "source": [
    "# MLP Fine-tuning Notebook\n",
    "\n",
    "This notebook fine-tunes a pre-trained MLP model by:\n",
    "1. Loading pre-trained weights from `mlp_dataset1_model.pth`\n",
    "2. Replacing the final head with a new one for the target dataset\n",
    "3. Fine-tuning on a new dataset (dataset2 or dataset3)\n",
    "\n",
    "The backbone layers are loaded from the pre-trained model, and only the head is replaced to match the number of classes in the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff05083",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ec982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanniattina/miniconda3/envs/task-vectors/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/giovanniattina/miniconda3/envs/task-vectors/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the MLP model\n",
    "from artificial_data.model_art import MLP\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8447e30",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Choose the dataset to fine-tune on and set training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6e4524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model: artificial_checkpoints/mlp_dataset1_model.pth\n",
      "Fine-tune dataset: dataset2\n",
      "Learning rate: 0.0001\n",
      "Batch size: 32\n",
      "Epochs: 50\n",
      "Freeze backbone: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "PRETRAINED_MODEL_PATH = \"artificial_checkpoints/mlp_dataset1_model.pth\"\n",
    "FINETUNE_DATASET = \"dataset2\"  # Change this to \"dataset2\" or \"dataset3\"\n",
    "\n",
    "# Fine-tuning parameters\n",
    "FINETUNE_LEARNING_RATE = 0.0001  # Lower learning rate for fine-tuning\n",
    "FINETUNE_BATCH_SIZE = 32\n",
    "FINETUNE_EPOCHS = 50\n",
    "\n",
    "# Freezing strategy\n",
    "FREEZE_BACKBONE = False  # Set to True to freeze backbone layers, False to train all layers\n",
    "\n",
    "print(f\"Pre-trained model: {PRETRAINED_MODEL_PATH}\")\n",
    "print(f\"Fine-tune dataset: {FINETUNE_DATASET}\")\n",
    "print(f\"Learning rate: {FINETUNE_LEARNING_RATE}\")\n",
    "print(f\"Batch size: {FINETUNE_BATCH_SIZE}\")\n",
    "print(f\"Epochs: {FINETUNE_EPOCHS}\")\n",
    "print(f\"Freeze backbone: {FREEZE_BACKBONE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e4e78",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model\n",
    "\n",
    "Load the pre-trained model weights and examine the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261a169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded pre-trained model checkpoint\n",
      "Original model - Input dim: 20, Output dim: 2\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model checkpoint\n",
    "try:\n",
    "    checkpoint = torch.load(PRETRAINED_MODEL_PATH, map_location=device)\n",
    "    print(\"✓ Successfully loaded pre-trained model checkpoint\")\n",
    "    \n",
    "    # Check if it's a complete checkpoint or just state dict\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        pretrained_state_dict = checkpoint['model_state_dict']\n",
    "        model_config = checkpoint.get('model_config', {})\n",
    "        training_config = checkpoint.get('training_config', {})\n",
    "        \n",
    "        print(f\"Pre-trained model config: {model_config}\")\n",
    "        print(f\"Original training config: {training_config}\")\n",
    "        \n",
    "        # Get original model dimensions\n",
    "        original_input_dim = model_config.get('input_dim', 20)  # Default based on dataset1\n",
    "        original_output_dim = model_config.get('output_dim', 2)  # Default binary classification\n",
    "    else:\n",
    "        # If it's just a state dict\n",
    "        pretrained_state_dict = checkpoint\n",
    "        # Infer dimensions from the state dict\n",
    "        original_input_dim = pretrained_state_dict['layers.0.weight'].shape[1]\n",
    "        original_output_dim = pretrained_state_dict['head.2.weight'].shape[0]\n",
    "    \n",
    "    print(f\"Original model - Input dim: {original_input_dim}, Output dim: {original_output_dim}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Could not find pre-trained model at {PRETRAINED_MODEL_PATH}\")\n",
    "    print(\"Please ensure the model file exists and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading pre-trained model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4cb02",
   "metadata": {},
   "source": [
    "## 4. Load Fine-tuning Dataset\n",
    "\n",
    "Load the new dataset for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aea11cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuning dataset: dataset2\n",
      "Train file: artificial_datasets/dataset2_train.csv\n",
      "Test file: artificial_datasets/dataset2_test.csv\n",
      "✓ Training data shape: (40000, 31)\n",
      "✓ Validation data shape: (10000, 31)\n",
      "\n",
      "Dataset info:\n",
      "Feature columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29']\n",
      "Target column: target\n",
      "Unique classes: [0, 1, 2, 3, 4]\n",
      "Number of classes: 5\n",
      "Input dimension: 30\n",
      "\n",
      "Class distribution in training set:\n",
      "  Class 0: 8014 samples (20.0%)\n",
      "  Class 1: 7976 samples (19.9%)\n",
      "  Class 2: 8020 samples (20.1%)\n",
      "  Class 3: 7994 samples (20.0%)\n",
      "  Class 4: 7996 samples (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuning dataset\n",
    "finetune_train_path = f\"artificial_datasets/{FINETUNE_DATASET}_train.csv\"\n",
    "finetune_test_path = f\"artificial_datasets/{FINETUNE_DATASET}_test.csv\"\n",
    "\n",
    "print(f\"Loading fine-tuning dataset: {FINETUNE_DATASET}\")\n",
    "print(f\"Train file: {finetune_train_path}\")\n",
    "print(f\"Test file: {finetune_test_path}\")\n",
    "\n",
    "try:\n",
    "    # Load training data\n",
    "    finetune_train_data = pd.read_csv(finetune_train_path)\n",
    "    print(f\"✓ Training data shape: {finetune_train_data.shape}\")\n",
    "    \n",
    "    # Load validation data\n",
    "    finetune_val_data = pd.read_csv(finetune_test_path)\n",
    "    print(f\"✓ Validation data shape: {finetune_val_data.shape}\")\n",
    "    \n",
    "    # Examine the data\n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"Feature columns: {finetune_train_data.columns[:-1].tolist()}\")\n",
    "    print(f\"Target column: {finetune_train_data.columns[-1]}\")\n",
    "    \n",
    "    # Check number of classes in new dataset\n",
    "    unique_classes = sorted(finetune_train_data['target'].unique())\n",
    "    new_num_classes = len(unique_classes)\n",
    "    new_input_dim = finetune_train_data.shape[1] - 1  # Exclude target column\n",
    "    \n",
    "    print(f\"Unique classes: {unique_classes}\")\n",
    "    print(f\"Number of classes: {new_num_classes}\")\n",
    "    print(f\"Input dimension: {new_input_dim}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    print(f\"\\nClass distribution in training set:\")\n",
    "    class_counts = finetune_train_data['target'].value_counts().sort_index()\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"  Class {cls}: {count} samples ({count/len(finetune_train_data)*100:.1f}%)\")\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: Could not find dataset files. Please check the dataset name.\")\n",
    "    print(f\"Available datasets: dataset1, dataset2, dataset3\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26873b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning data tensors:\n",
      "  Training features: torch.Size([40000, 30])\n",
      "  Training targets: torch.Size([40000])\n",
      "  Validation features: torch.Size([10000, 30])\n",
      "  Validation targets: torch.Size([10000])\n",
      "\n",
      "DataLoaders created:\n",
      "  Training batches: 1250\n",
      "  Validation batches: 313\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for fine-tuning\n",
    "X_finetune_train = finetune_train_data.iloc[:, :-1].values\n",
    "y_finetune_train = finetune_train_data.iloc[:, -1].values\n",
    "\n",
    "X_finetune_val = finetune_val_data.iloc[:, :-1].values\n",
    "y_finetune_val = finetune_val_data.iloc[:, -1].values\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_finetune_train_tensor = torch.FloatTensor(X_finetune_train).to(device)\n",
    "y_finetune_train_tensor = torch.LongTensor(y_finetune_train).to(device)\n",
    "\n",
    "X_finetune_val_tensor = torch.FloatTensor(X_finetune_val).to(device)\n",
    "y_finetune_val_tensor = torch.LongTensor(y_finetune_val).to(device)\n",
    "\n",
    "print(f\"Fine-tuning data tensors:\")\n",
    "print(f\"  Training features: {X_finetune_train_tensor.shape}\")\n",
    "print(f\"  Training targets: {y_finetune_train_tensor.shape}\")\n",
    "print(f\"  Validation features: {X_finetune_val_tensor.shape}\")\n",
    "print(f\"  Validation targets: {y_finetune_val_tensor.shape}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "finetune_train_dataset = TensorDataset(X_finetune_train_tensor, y_finetune_train_tensor)\n",
    "finetune_val_dataset = TensorDataset(X_finetune_val_tensor, y_finetune_val_tensor)\n",
    "\n",
    "finetune_train_loader = DataLoader(finetune_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True)\n",
    "finetune_val_loader = DataLoader(finetune_val_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Training batches: {len(finetune_train_loader)}\")\n",
    "print(f\"  Validation batches: {len(finetune_val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf35edc",
   "metadata": {},
   "source": [
    "## 5. Create Fine-tuned Model with New Head\n",
    "\n",
    "Create a model with pre-trained backbone and new head for the target dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ab8f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fine-tuned model...\n",
      "  Input dimension: 30\n",
      "  Output dimension: 5\n",
      "\n",
      "Model architecture:\n",
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (19): ReLU()\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=15, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=15, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Loading pre-trained weights...\n",
      "  ✗ Skipped: layers.0.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.0.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.2.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.2.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.4.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.4.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.6.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.6.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.8.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.8.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.10.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.10.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.12.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.12.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.14.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.14.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.16.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.16.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ✗ Skipped: layers.18.weight (shape mismatch: torch.Size([30, 30]) vs torch.Size([20, 20]))\n",
      "  ✗ Skipped: layers.18.bias (shape mismatch: torch.Size([30]) vs torch.Size([20]))\n",
      "  ⚠ Skipped head layer: head.0.weight (will be randomly initialized)\n",
      "  ⚠ Skipped head layer: head.0.bias (will be randomly initialized)\n",
      "  ⚠ Skipped head layer: head.2.weight (will be randomly initialized)\n",
      "  ⚠ Skipped head layer: head.2.bias (will be randomly initialized)\n",
      "\n",
      "✓ Successfully loaded 0 pre-trained layers\n",
      "✓ Head layers initialized randomly for 5 classes\n",
      "\n",
      "🔓 All layers will be trained (no freezing)\n",
      "\n",
      "Parameter summary:\n",
      "  Total parameters: 9,845\n",
      "  Trainable parameters: 9,845\n",
      "  Frozen parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# Create new model with the same input dimension but new output dimension\n",
    "print(f\"Creating fine-tuned model...\")\n",
    "print(f\"  Input dimension: {new_input_dim}\")\n",
    "print(f\"  Output dimension: {new_num_classes}\")\n",
    "\n",
    "# Create the fine-tuned model\n",
    "finetuned_model = MLP(input_dim=new_input_dim, output_dim=new_num_classes).to(device)\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(finetuned_model)\n",
    "\n",
    "# Load the pre-trained backbone weights (excluding the head)\n",
    "print(f\"\\nLoading pre-trained weights...\")\n",
    "model_dict = finetuned_model.state_dict()\n",
    "pretrained_dict = {}\n",
    "\n",
    "# Filter out the head layers and load only backbone layers\n",
    "for k, v in pretrained_state_dict.items():\n",
    "    if k.startswith('layers.') and k in model_dict:\n",
    "        # Only load if dimensions match\n",
    "        if model_dict[k].shape == v.shape:\n",
    "            pretrained_dict[k] = v\n",
    "            print(f\"  ✓ Loaded: {k} {v.shape}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Skipped: {k} (shape mismatch: {model_dict[k].shape} vs {v.shape})\")\n",
    "    elif k.startswith('head.'):\n",
    "        print(f\"  ⚠ Skipped head layer: {k} (will be randomly initialized)\")\n",
    "\n",
    "# Update the model dict with pre-trained weights\n",
    "model_dict.update(pretrained_dict)\n",
    "finetuned_model.load_state_dict(model_dict)\n",
    "\n",
    "print(f\"\\n✓ Successfully loaded {len(pretrained_dict)} pre-trained layers\")\n",
    "print(f\"✓ Head layers initialized randomly for {new_num_classes} classes\")\n",
    "\n",
    "# Freeze backbone layers if specified\n",
    "if FREEZE_BACKBONE:\n",
    "    print(f\"\\n🔒 Freezing backbone layers...\")\n",
    "    for name, param in finetuned_model.named_parameters():\n",
    "        if name.startswith('layers.'):\n",
    "            param.requires_grad = False\n",
    "            print(f\"  Frozen: {name}\")\n",
    "        else:\n",
    "            print(f\"  Trainable: {name}\")\n",
    "else:\n",
    "    print(f\"\\n🔓 All layers will be trained (no freezing)\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in finetuned_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nParameter summary:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Frozen parameters: {frozen_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fea430",
   "metadata": {},
   "source": [
    "## 6. Fine-tune the Model\n",
    "\n",
    "Train the model on the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2203b1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "  Learning rate: 0.0001\n",
      "  Batch size: 32\n",
      "  Epochs: 50\n",
      "  Freeze backbone: False\n",
      "------------------------------------------------------------\n",
      "Epoch [  1/50] | Train Loss: 1.4997 | Val Loss: 1.2989 | Val Accuracy: 40.01% | Best: 40.01%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(finetune_train_loader):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuned_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Developer/deeplearning/task_vectors/model_art.py:20\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 20\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(finetuned_model.parameters(), lr=FINETUNE_LEARNING_RATE)\n",
    "\n",
    "# Training tracking\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Learning rate: {FINETUNE_LEARNING_RATE}\")\n",
    "print(f\"  Batch size: {FINETUNE_BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {FINETUNE_EPOCHS}\")\n",
    "print(f\"  Freeze backbone: {FREEZE_BACKBONE}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(FINETUNE_EPOCHS):\n",
    "    # Training phase\n",
    "    finetuned_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_X, batch_y in finetune_train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = finetuned_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # Validation phase\n",
    "    finetuned_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in finetune_val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = finetuned_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    epoch_train_loss = train_loss / len(finetune_train_loader)\n",
    "    epoch_train_acc = 100 * train_correct / train_total\n",
    "    epoch_val_loss = val_loss / len(finetune_val_loader)\n",
    "    epoch_val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = epoch_val_acc\n",
    "        best_model_state = copy.deepcopy(finetuned_model.state_dict())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{FINETUNE_EPOCHS}]:')\n",
    "        print(f'  Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%')\n",
    "        print(f'  Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
    "\n",
    "# Load best model\n",
    "finetuned_model.load_state_dict(best_model_state)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Fine-tuning completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2be4ba",
   "metadata": {},
   "source": [
    "## 6. Evaluate Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "finetuned_model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in finetune_val_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = finetuned_model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "final_accuracy = accuracy_score(all_targets, all_predictions)\n",
    "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_predictions))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - Fine-tuned Model\\n{FINETUNE_DATASET}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "unique_classes = sorted(list(set(all_targets)))\n",
    "print(f\"\\nPer-class accuracy:\")\n",
    "for cls in unique_classes:\n",
    "    cls_mask = np.array(all_targets) == cls\n",
    "    cls_acc = accuracy_score(np.array(all_targets)[cls_mask], np.array(all_predictions)[cls_mask])\n",
    "    print(f\"Class {cls}: {cls_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a65ca7",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(range(1, FINETUNE_EPOCHS + 1), train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(range(1, FINETUNE_EPOCHS + 1), val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_title(f'Fine-tuning Loss - {FINETUNE_DATASET}')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(range(1, FINETUNE_EPOCHS + 1), train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(range(1, FINETUNE_EPOCHS + 1), val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_title(f'Fine-tuning Accuracy - {FINETUNE_DATASET}')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best metrics\n",
    "best_val_acc_epoch = np.argmax(val_accuracies) + 1\n",
    "best_val_acc = max(val_accuracies)\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_val_acc_epoch}\")\n",
    "\n",
    "# Training summary\n",
    "print(f\"\\nFine-tuning Summary:\")\n",
    "print(f\"- Dataset: {FINETUNE_DATASET}\")\n",
    "print(f\"- Learning Rate: {FINETUNE_LEARNING_RATE}\")\n",
    "print(f\"- Batch Size: {FINETUNE_BATCH_SIZE}\")\n",
    "print(f\"- Epochs: {FINETUNE_EPOCHS}\")\n",
    "print(f\"- Backbone Frozen: {FREEZE_BACKBONE}\")\n",
    "print(f\"- Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"- Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "print(f\"- Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5eed61",
   "metadata": {},
   "source": [
    "## 8. Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "import os\n",
    "os.makedirs('artificial_checkpoints', exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "finetuned_model_path = f'artificial_checkpoints/mlp_{FINETUNE_DATASET}_finetuned_model.pth'\n",
    "finetuned_head_path = f'artificial_checkpoints/mlp_{FINETUNE_DATASET}_finetuned_head.pth'\n",
    "\n",
    "# Save complete model\n",
    "torch.save(finetuned_model.state_dict(), finetuned_model_path)\n",
    "print(f\"Fine-tuned model saved to: {finetuned_model_path}\")\n",
    "\n",
    "# Save only the head (for potential future use)\n",
    "head_state_dict = {k: v for k, v in finetuned_model.state_dict().items() if 'head' in k}\n",
    "torch.save(head_state_dict, finetuned_head_path)\n",
    "print(f\"Fine-tuned head saved to: {finetuned_head_path}\")\n",
    "\n",
    "# Save training metadata\n",
    "metadata = {\n",
    "    'original_model_path': PRETRAINED_MODEL_PATH,\n",
    "    'finetune_dataset': FINETUNE_DATASET,\n",
    "    'learning_rate': FINETUNE_LEARNING_RATE,\n",
    "    'batch_size': FINETUNE_BATCH_SIZE,\n",
    "    'epochs': FINETUNE_EPOCHS,\n",
    "    'freeze_backbone': FREEZE_BACKBONE,\n",
    "    'input_dim': new_input_dim,\n",
    "    'num_classes': new_num_classes,\n",
    "    'trainable_parameters': trainable_params,\n",
    "    'total_parameters': total_params\n",
    "}\n",
    "\n",
    "# If training was completed, add final metrics\n",
    "if 'finetune_val_accuracies' in locals() and len(finetune_val_accuracies) > 0:\n",
    "    metadata.update({\n",
    "        'final_val_accuracy': finetune_val_accuracies[-1],\n",
    "        'best_val_accuracy': max(finetune_val_accuracies),\n",
    "        'best_epoch': finetune_val_accuracies.index(max(finetune_val_accuracies)) + 1\n",
    "    })\n",
    "\n",
    "metadata_path = f'artificial_checkpoints/mlp_{FINETUNE_DATASET}_finetuned_metadata.txt'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    for key, value in metadata.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(f\"Training metadata saved to: {metadata_path}\")\n",
    "print(f\"\\nFine-tuning experiment completed!\")\n",
    "print(f\"- Original model: {PRETRAINED_MODEL_PATH}\")\n",
    "print(f\"- Fine-tune dataset: {FINETUNE_DATASET}\")\n",
    "print(f\"- Classes in new dataset: {new_num_classes}\")\n",
    "print(f\"- Backbone frozen: {FREEZE_BACKBONE}\")\n",
    "print(f\"- Saved model: {finetuned_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task-vectors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
