{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06ca4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee6de8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "\n",
    "from src.task_vectors_adapted import TaskVectorAdapted\n",
    "from src.eval import eval_single_dataset\n",
    "from src.args import parse_arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8bba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "datasets = ['MNIST', 'cars']\n",
    "model = 'ViT-L-14'\n",
    "\n",
    "sys.argv = [\n",
    "    'notebook',  # script name\n",
    "    '--data-location', '../datasets',\n",
    "    '--model', f'../{model}',\n",
    "    '--save', f'../checkpoints/{model}',\n",
    "    '--batch-size', '128'\n",
    "]\n",
    "args = parse_arguments()\n",
    "\n",
    "\n",
    "pretrained_checkpoint = f'../checkpoints/{model}/zeroshot.pt'\n",
    "finetuned_checkpoints = [f'../checkpoints/{model}/{dataset}/finetuned.pt' for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc884f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = ['subtract', 'add', 'multiply', 'divide']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56ff619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset: MNIST\n",
      "Testing operation: subtract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanniattina/miniconda3/envs/task-vectors/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification head for ../ViT-L-14 on MNIST exists at ../checkpoints/ViT-L-14/head_MNIST.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_MNIST.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:36<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 100.00%\n",
      "Testing operation: add\n",
      "Classification head for ../ViT-L-14 on MNIST exists at ../checkpoints/ViT-L-14/head_MNIST.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_MNIST.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 12.50%\n",
      "Testing operation: multiply\n",
      "Classification head for ../ViT-L-14 on MNIST exists at ../checkpoints/ViT-L-14/head_MNIST.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_MNIST.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 9.38%\n",
      "Testing operation: divide\n",
      "Classification head for ../ViT-L-14 on MNIST exists at ../checkpoints/ViT-L-14/head_MNIST.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_MNIST.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:34<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 7.81%\n",
      "Testing dataset: Cars\n",
      "Testing operation: subtract\n",
      "Classification head for ../ViT-L-14 on Cars exists at ../checkpoints/ViT-L-14/head_Cars.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_Cars.pt\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting operation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m encoder_test \u001b[38;5;241m=\u001b[39m task_vector\u001b[38;5;241m.\u001b[39mapply_to(pretrained_checkpoint, scaling_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43meval_single_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m results[(dataset, operation)] \u001b[38;5;241m=\u001b[39m metrics\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m task_vector\n",
      "File \u001b[0;32m~/Developer/deeplearning/task_vectors/src/eval.py:22\u001b[0m, in \u001b[0;36meval_single_dataset\u001b[0;34m(image_encoder, dataset_name, args)\u001b[0m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m ImageClassifier(image_encoder, classification_head)\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 22\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_preprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m get_dataloader(\n\u001b[1;32m     29\u001b[0m     dataset, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, args\u001b[38;5;241m=\u001b[39margs, image_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m device \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/Developer/deeplearning/task_vectors/src/datasets/registry.py:97\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(dataset_name, preprocess, location, batch_size, num_workers, val_fraction, max_val_samples)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m registry, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Supported datasets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(registry\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     96\u001b[0m     dataset_class \u001b[38;5;241m=\u001b[39m registry[dataset_name]\n\u001b[0;32m---> 97\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/Developer/deeplearning/task_vectors/src/datasets/cars.py:138\u001b[0m, in \u001b[0;36mCars.__init__\u001b[0;34m(self, preprocess, location, batch_size, num_workers)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    132\u001b[0m              preprocess,\n\u001b[1;32m    133\u001b[0m              location\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/data\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    134\u001b[0m              batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    135\u001b[0m              num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Data loading code\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPytorchStanfordCars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[1;32m    141\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    143\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset \u001b[38;5;241m=\u001b[39m PytorchStanfordCars(location, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocess, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Developer/deeplearning/task_vectors/src/datasets/cars.py:65\u001b[0m, in \u001b[0;36mPytorchStanfordCars.__init__\u001b[0;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_images_base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcars_test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/deeplearning/task_vectors/src/datasets/cars.py:100\u001b[0m, in \u001b[0;36mPytorchStanfordCars.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://ai.stanford.edu/~jkrause/cars/car_devkit.tgz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc3b158d763b6e2245038c8ad08e45376\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     download_and_extract_archive(\n\u001b[1;32m    107\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://ai.stanford.edu/~jkrause/car196/cars_train.tgz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m         download_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_folder),\n\u001b[1;32m    109\u001b[0m         md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m065e5b463ae28d29e77c1b4b166cfe61\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/site-packages/torchvision/datasets/utils.py:446\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    444\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 446\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/site-packages/torchvision/datasets/utils.py:146\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    143\u001b[0m     _download_file_from_remote_location(fpath, url)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# expand redirect chain if needed\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43m_get_redirect_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_hops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_redirect_hops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# check if file is located on Google Drive\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     file_id \u001b[38;5;241m=\u001b[39m _get_google_drive_file_id(url)\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/site-packages/torchvision/datasets/utils.py:94\u001b[0m, in \u001b[0;36m_get_redirect_url\u001b[0;34m(url, max_hops)\u001b[0m\n\u001b[1;32m     91\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT}\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_hops \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m==\u001b[39m url \u001b[38;5;129;01mor\u001b[39;00m response\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/task-vectors/lib/python3.10/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for dataset in datasets:\n",
    "    print(f\"Testing dataset: {dataset}\")\n",
    "\n",
    "    finetuned_checkpoint = f'../checkpoints/{model}/{dataset}/finetuned.pt'\n",
    "    for operation in operations:\n",
    "        task_vector = TaskVectorAdapted(\n",
    "            pretrained_checkpoint=pretrained_checkpoint,\n",
    "            finetuned_checkpoint=finetuned_checkpoint,\n",
    "            operation=operation\n",
    "        )\n",
    "        print(f\"Testing operation: {operation}\")\n",
    "        encoder_test = task_vector.apply_to(pretrained_checkpoint, scaling_coef=0.5)\n",
    "        metrics = eval_single_dataset(encoder_test, dataset, args)\n",
    "        results[(dataset, operation)] = metrics\n",
    "\n",
    "        del task_vector\n",
    "        del encoder_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9491aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vectors = {\n",
    "    operation: TaskVectorAdapted(\n",
    "        pretrained_checkpoint=pretrained_checkpoint,\n",
    "        finetuned_checkpoint=finetuned_checkpoint,\n",
    "        operation=operation\n",
    "    )\n",
    "    for operation in operations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40fd7523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing operation: subtract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanniattina/miniconda3/envs/task-vectors/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification head for ../ViT-L-14 on MNIST exists at ../checkpoints/ViT-L-14/head_MNIST.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_MNIST.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:34<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 100.00%\n",
      "Testing operation: add\n",
      "Classification head for ../ViT-L-14 on MNIST exists at ../checkpoints/ViT-L-14/head_MNIST.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_MNIST.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 12.50%\n",
      "Testing operation: multiply\n",
      "Classification head for ../ViT-L-14 on MNIST exists at ../checkpoints/ViT-L-14/head_MNIST.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_MNIST.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:37<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 9.38%\n",
      "Testing operation: divide\n",
      "Classification head for ../ViT-L-14 on MNIST exists at ../checkpoints/ViT-L-14/head_MNIST.pt\n",
      "Loading classification head from ../checkpoints/ViT-L-14/head_MNIST.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:36<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 7.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for operation, task_vector in task_vectors.items():\n",
    "    print(f\"Testing operation: {operation}\")\n",
    "    encoder_test = task_vector.apply_to(pretrained_checkpoint, scaling_coef=0.5)\n",
    "    metrics = eval_single_dataset(encoder_test, dataset, args)\n",
    "    results[operation] = metrics\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f27d2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subtract': {'top1': 1.0},\n",
       " 'add': {'top1': 0.125},\n",
       " 'multiply': {'top1': 0.09375},\n",
       " 'divide': {'top1': 0.078125}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bff14e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cbc8ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "top1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "58ddcffd-7fdf-43c1-9925-a86c8b4315e4",
       "rows": [
        [
         "add",
         "0.125"
        ],
        [
         "divide",
         "0.078125"
        ],
        [
         "multiply",
         "0.09375"
        ],
        [
         "subtract",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divide</th>\n",
       "      <td>0.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiply</th>\n",
       "      <td>0.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subtract</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              top1\n",
       "add       0.125000\n",
       "divide    0.078125\n",
       "multiply  0.093750\n",
       "subtract  1.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17725e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task-vectors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
